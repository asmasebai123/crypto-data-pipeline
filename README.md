# Crypto Data Pipeline â€” End-to-End

Pipeline Data Engineering complet pour collecter, traiter et visualiser
les prix des cryptomonnaies en temps rÃ©el.

---

## Architecture
```
CoinGecko API â”€â”€(batch/10min)â”€â”€â”
                                â”œâ”€â”€â–º PostgreSQL â”€â”€â–º Transformations â”€â”€â–º Streamlit
Kafka Simulator â”€â”€(streaming)â”€â”€â”˜         â”‚              Pandas            Dashboard
                                         â”‚
                                      Prefect
                                   (orchestration)
```

---

## Stack technologique

| Couche | Technologie | Justification |
|---|---|---|
| Ingestion Batch | Python + CoinGecko API | API gratuite, fiable, donnÃ©es structurÃ©es |
| Ingestion Streaming | Apache Kafka | Standard industrie pour flux temps rÃ©el |
| Stockage | PostgreSQL | DonnÃ©es structurÃ©es, requÃªtes analytiques simples |
| Transformation | Pandas | Volume modÃ©rÃ©, dÃ©veloppement rapide |
| Orchestration | Prefect | Dashboard intÃ©grÃ©, plus simple qu'Airflow |
| Visualisation | Streamlit + Plotly | DÃ©ploiement rapide, interface interactive |
| DÃ©ploiement | Docker Compose | ReproductibilitÃ© garantie |

---

## Sources de donnÃ©es

- **Source 1 â€” Batch** : CoinGecko API publique
  - Endpoint : `/coins/markets`
  - FrÃ©quence : toutes les 10 minutes
  - Cryptos : Bitcoin, Ethereum, Solana, XRP, BNB

- **Source 2 â€” Streaming** : Apache Kafka
  - Topic : `crypto_prices`
  - Simulation de variations de prix en temps rÃ©el
  - Intervalle : toutes les 5 secondes

---

## Transformations mÃ©tier

1. **Nettoyage** â€” suppression valeurs nulles, doublons, prix aberrants
2. **Moyenne horaire** â€” prix moyen par crypto par heure
3. **Classement journalier** â€” ranking par performance 24h
4. **DÃ©tection alertes** â€” variation > Â±5% dÃ©clenche une alerte
5. **Volume journalier** â€” volume total Ã©changÃ© par jour

---

## Structure du projet
```
crypto_data_project/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ main.py              # Ingestion batch CoinGecko
â”‚   â”œâ”€â”€ database.py          # Connexion PostgreSQL + tables
â”‚   â”œâ”€â”€ kafka_producer.py    # Producteur Kafka
â”‚   â”œâ”€â”€ kafka_consumer.py    # Consommateur Kafka
â”‚   â””â”€â”€ fetch_history.py     # Historique rÃ©el CoinGecko
â”œâ”€â”€ transformations/
â”‚   â”œâ”€â”€ cleaning.py          # Nettoyage donnÃ©es
â”‚   â”œâ”€â”€ aggregations.py      # Moyenne horaire + volume
â”‚   â”œâ”€â”€ rankings.py          # Classement performance
â”‚   â”œâ”€â”€ alerts.py            # DÃ©tection alertes Â±5%
â”‚   â””â”€â”€ run_transforms.py    # Pipeline transformations
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ flows.py             # Flows Prefect
â”‚   â””â”€â”€ scheduler.py         # Planification 10 minutes
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py               # Dashboard Streamlit
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transformations.py  # 18 tests unitaires
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Installation et dÃ©marrage

### PrÃ©requis
- Docker Desktop installÃ© et dÃ©marrÃ©
- Python 3.10+
- ClÃ© API CoinGecko gratuite : https://www.coingecko.com/en/api

### Ã‰tape 1 â€” Cloner le projet
```bash
git clone https://github.com/TON_PSEUDO/crypto-data-pipeline.git
cd crypto-data-pipeline
```

### Ã‰tape 2 â€” Configurer l'environnement
```bash
# CrÃ©er l'environnement virtuel
python -m venv venv
venv\Scripts\activate        # Windows
source venv/bin/activate     # Linux/Mac

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer les variables
cp .env.example .env
# Editer .env et ajouter ta clÃ© CoinGecko
```

### Ã‰tape 3 â€” DÃ©marrer les services Docker
```bash
docker compose up -d
```

### Ã‰tape 4 â€” Initialiser la base de donnÃ©es
```bash
python ingestion/database.py
```

### Ã‰tape 5 â€” RÃ©cupÃ©rer les donnÃ©es historiques
```bash
python ingestion/fetch_history.py
```

### Ã‰tape 6 â€” Lancer le pipeline complet
```bash
python orchestration/flows.py
```

### Ã‰tape 7 â€” Lancer le dashboard
```bash
streamlit run dashboard/app.py
```

Dashboard accessible sur : **http://localhost:8501**

---

## Lancer les tests
```bash
pytest tests/ -v
```
RÃ©sultat attendu : **18 tests passÃ©s**

---

## DÃ©marrage rapide (tout en un)
```bash
# Windows â€” double-cliquer sur start.bat
docker compose up -d
python orchestration/flows.py
streamlit run dashboard/app.py
```

---

## Logs
Les logs sont sauvegardÃ©s dans le dossier `logs/` :
- `logs/ingestion.log` â€” ingestion batch
- `logs/kafka_producer.log` â€” producteur Kafka
- `logs/kafka_consumer.log` â€” consommateur Kafka
- `logs/transformations.log` â€” transformations

---

## DifficultÃ©s rencontrÃ©es
- **SSL Certificate Error** : rÃ©seau universitaire avec proxy
  â†’ Solution : `verify=False` + `urllib3.disable_warnings`
- **CoinGecko 403 Forbidden** : clÃ© API requise depuis 2024
  â†’ Solution : inscription gratuite sur coingecko.com
- **kafka-python incompatible Python 3.12**
  â†’ Solution : migration vers `kafka-python-ng`

  ---
  ## ğŸŒ Dashboard en ligne

**URL publique** : https://crypto-data-pipeline-8mrtpexxnwydrp9dbehvt5.streamlit.app

Le dashboard est accessible 24/7 et se met Ã  jour automatiquement.
